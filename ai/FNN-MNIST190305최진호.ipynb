{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 데이터 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-76cba88990fd>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/pirl/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/pirl/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/pirl/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/pirl/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/pirl/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)\n",
    "train_data = mnist.train.images\n",
    "train_label = mnist.train.labels\n",
    "test_data = mnist.test.images\n",
    "test_label = mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 데이터 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# size of MNIST\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing (데이터 전처리) : 다른 데이터로 하게 되면 해야됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation (1)\n",
    "\n",
    "\n",
    "## Loss function (손실 함수) : Cross Entropy\n",
    "\n",
    "# <center> \\\\( L(y_i, f(x_i; W)) = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{k=1} y_{i,j} log(f(x_i)_k)\\\\)</center>\n",
    "\n",
    "\n",
    "#### get_cross_entropy_loss 함수의 내용을 완성하세요.\n",
    "#### (Hint : (1) tf.reduce_mean(), tf.reduce_sum(), tf.log() (2) Tensor dimension에 유의 (3) log 함수 사용 시 epsilon 사용하세요.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_entropy_loss(y_true, y_hat, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    compute cross entropy\n",
    "\n",
    "    Args:\n",
    "        y_true: true label\n",
    "        y_hat: predicted label\n",
    "        epsilon: small value to prevent NaN in log\n",
    "\n",
    "    Returns:\n",
    "        cross entropy loss\n",
    "    \"\"\"\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "#         loss = -tf.reduce_mean(tf.reduce_sum(y_true * tf.log(y_hat), axis=1)) #axis = 0은 세로, 1은 가로 행 하나를 집어서 열을 다 합침\n",
    "        loss = -tf.reduce_mean(tf.reduce_sum(y_true * tf.log(y_hat), axis=1))\n",
    "        return loss\n",
    "    \n",
    "# y_true = tf.placeholder(tf.float32, [None, 10]) # 샘플 갯수랑, 디멘젼\n",
    "# y_hat = tf.placeholder(tf.float32, [None, 10]) # 샘플 갯수랑 디멘젼\n",
    "# get_cross_entropy_loss(y_true, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "\n",
    "def get_accuracy(y_true, y_hat):\n",
    "    \"\"\"\n",
    "    compute cross entropy\n",
    "\n",
    "    Args:\n",
    "        y_true: true label\n",
    "        y_hat: predicted label\n",
    "        epsilon: small value to prevent NaN in log\n",
    "\n",
    "    Returns:\n",
    "        cross entropy loss\n",
    "    \"\"\"\n",
    "    with tf.name_scope('accuracy'):\n",
    "        # Compare the highest indices between the predicted label and the true label\n",
    "        correct_prediction = tf.equal(tf.argmax(y_hat, 1), tf.argmax(y_true, 1), name='correct_prediction')\n",
    "        # Compute accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter (하이퍼 파라미터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hypyerparameters\n",
    "learning_rate = 0.005\n",
    "max_iter = 5000\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation (2)\n",
    "## Linear Classifier (선형 분류기)\n",
    "\n",
    "## <center> \\\\( f(x) = W^Tx+b \\\\)</center>\n",
    "\n",
    "### linear function을 완성하세요.\n",
    "### (Hint : (1) weight, bias 선언 (2) tf.get_variable()의 initializer  (3) tf.matmul())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(name, out_dim, inputs):\n",
    "    \"\"\"\n",
    "    Args :\n",
    "        Inputs : Input tensor\n",
    "        out_dim : output dimension\n",
    "        \n",
    "    Returns:\n",
    "        inputs * weight + bias\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        shp = inputs.get_shape().as_list()[1]\n",
    "        initial = tf.truncated_normal([shp, out_dim], stddev=.01)\n",
    "        weights = tf.get_variable('w', initializer=initial) #reuse하지 않는 이상 \n",
    "        y_hat = tf.matmul(inputs, weights)\n",
    "        return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = tf.placeholder(tf.float32, [None,784])\n",
    "# a.get_shape().as_list()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation (3)\n",
    "\n",
    "## Model Setting\n",
    "\n",
    "### 1. Training data 및 Test data의 각각의 image를 한 vector로 만들어서 train_data, test_data에 각각 저장하세요.\n",
    "#### Hint) 데이터 차원.\n",
    "### 2. Dataset로부터 받은 데이터(Image, label)를 담을 변수를 각각 x 및 y_true에 선언하세요.\n",
    "#### Hint) tf.placeholder\n",
    "### 3. Implementation (2)에서 구현한 linear classifier 함수값과 softmax 함수를 통한 prediction 값을 y_hat에 저장하세요.\n",
    "#### Hint) tf.nn.softmax\n",
    "### 4. 3으로부터 얻은 결과를 통해 Implementation (1)에서 구현한 loss function을 통해 얻은 loss를 cross_entropy에 저장하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10th iteration, loss: 0.6187, test accuracy: 0.8080\n",
      "20th iteration, loss: 0.4590, test accuracy: 0.8560\n",
      "30th iteration, loss: 0.7180, test accuracy: 0.8518\n",
      "40th iteration, loss: 0.6949, test accuracy: 0.8606\n",
      "50th iteration, loss: 0.4075, test accuracy: 0.8606\n",
      "60th iteration, loss: 0.3394, test accuracy: 0.8695\n",
      "70th iteration, loss: 0.3967, test accuracy: 0.8741\n",
      "80th iteration, loss: 0.5362, test accuracy: 0.8810\n",
      "90th iteration, loss: 0.3647, test accuracy: 0.8826\n",
      "100th iteration, loss: 0.2775, test accuracy: 0.8824\n",
      "110th iteration, loss: 0.6166, test accuracy: 0.8823\n",
      "120th iteration, loss: 0.3619, test accuracy: 0.8852\n",
      "130th iteration, loss: 0.4663, test accuracy: 0.8794\n",
      "140th iteration, loss: 0.2061, test accuracy: 0.8914\n",
      "150th iteration, loss: 0.4213, test accuracy: 0.8983\n",
      "160th iteration, loss: 0.3478, test accuracy: 0.8738\n",
      "170th iteration, loss: 0.4540, test accuracy: 0.8800\n",
      "180th iteration, loss: 0.5929, test accuracy: 0.8926\n",
      "190th iteration, loss: 0.3909, test accuracy: 0.8784\n",
      "200th iteration, loss: 0.3271, test accuracy: 0.8941\n",
      "210th iteration, loss: 0.4369, test accuracy: 0.9065\n",
      "220th iteration, loss: 0.2670, test accuracy: 0.8975\n",
      "230th iteration, loss: 0.3917, test accuracy: 0.8966\n",
      "240th iteration, loss: 0.3268, test accuracy: 0.9000\n",
      "250th iteration, loss: 0.5082, test accuracy: 0.9046\n",
      "260th iteration, loss: 0.2724, test accuracy: 0.8862\n",
      "270th iteration, loss: 0.2671, test accuracy: 0.8807\n",
      "280th iteration, loss: 0.5302, test accuracy: 0.8738\n",
      "290th iteration, loss: 0.4931, test accuracy: 0.8928\n",
      "300th iteration, loss: 0.3874, test accuracy: 0.8882\n",
      "310th iteration, loss: 0.3762, test accuracy: 0.8703\n",
      "320th iteration, loss: 0.3738, test accuracy: 0.8851\n",
      "330th iteration, loss: 0.4025, test accuracy: 0.8968\n",
      "340th iteration, loss: 0.4927, test accuracy: 0.8800\n",
      "350th iteration, loss: 0.4847, test accuracy: 0.8963\n",
      "360th iteration, loss: 0.5149, test accuracy: 0.8821\n",
      "370th iteration, loss: 0.2263, test accuracy: 0.8938\n",
      "380th iteration, loss: 0.6278, test accuracy: 0.8902\n",
      "390th iteration, loss: 0.3404, test accuracy: 0.8910\n",
      "400th iteration, loss: 0.5864, test accuracy: 0.9015\n",
      "410th iteration, loss: 0.4241, test accuracy: 0.8863\n",
      "420th iteration, loss: 0.2718, test accuracy: 0.8849\n",
      "430th iteration, loss: 0.6922, test accuracy: 0.9029\n",
      "440th iteration, loss: 0.1918, test accuracy: 0.8956\n",
      "450th iteration, loss: 0.2257, test accuracy: 0.9025\n",
      "460th iteration, loss: 0.2639, test accuracy: 0.8858\n",
      "470th iteration, loss: 0.5999, test accuracy: 0.8860\n",
      "480th iteration, loss: 0.4273, test accuracy: 0.8951\n",
      "490th iteration, loss: 0.4647, test accuracy: 0.8980\n",
      "500th iteration, loss: 0.4624, test accuracy: 0.8981\n",
      "510th iteration, loss: 0.3918, test accuracy: 0.8976\n",
      "520th iteration, loss: 0.2838, test accuracy: 0.8974\n",
      "530th iteration, loss: 0.5692, test accuracy: 0.8859\n",
      "540th iteration, loss: 0.3543, test accuracy: 0.8979\n",
      "550th iteration, loss: 0.2407, test accuracy: 0.8989\n",
      "560th iteration, loss: 0.2907, test accuracy: 0.8812\n",
      "570th iteration, loss: 0.3744, test accuracy: 0.9022\n",
      "580th iteration, loss: 0.3806, test accuracy: 0.9040\n",
      "590th iteration, loss: 0.2456, test accuracy: 0.9057\n",
      "600th iteration, loss: 0.4281, test accuracy: 0.8940\n",
      "610th iteration, loss: 0.3810, test accuracy: 0.8976\n",
      "620th iteration, loss: 0.5525, test accuracy: 0.9019\n",
      "630th iteration, loss: 0.2634, test accuracy: 0.8867\n",
      "640th iteration, loss: 0.3818, test accuracy: 0.9020\n",
      "650th iteration, loss: 0.3451, test accuracy: 0.8927\n",
      "660th iteration, loss: 0.4738, test accuracy: 0.9021\n",
      "670th iteration, loss: 0.4205, test accuracy: 0.8957\n",
      "680th iteration, loss: 0.6525, test accuracy: 0.9064\n",
      "690th iteration, loss: 0.6366, test accuracy: 0.9033\n",
      "700th iteration, loss: 0.2574, test accuracy: 0.8866\n",
      "710th iteration, loss: 0.4442, test accuracy: 0.8943\n",
      "720th iteration, loss: 0.5508, test accuracy: 0.9069\n",
      "730th iteration, loss: 0.2811, test accuracy: 0.9080\n",
      "740th iteration, loss: 0.4918, test accuracy: 0.8825\n",
      "750th iteration, loss: 0.2444, test accuracy: 0.9079\n",
      "760th iteration, loss: 0.3780, test accuracy: 0.9091\n",
      "770th iteration, loss: 0.5247, test accuracy: 0.8985\n",
      "780th iteration, loss: 0.2763, test accuracy: 0.8768\n",
      "790th iteration, loss: 0.5998, test accuracy: 0.8886\n",
      "800th iteration, loss: 0.2146, test accuracy: 0.9027\n",
      "810th iteration, loss: 0.3157, test accuracy: 0.9114\n",
      "820th iteration, loss: 0.3226, test accuracy: 0.9008\n",
      "830th iteration, loss: 0.3394, test accuracy: 0.9007\n",
      "840th iteration, loss: 0.5933, test accuracy: 0.9017\n",
      "850th iteration, loss: 0.2560, test accuracy: 0.8772\n",
      "860th iteration, loss: 0.4236, test accuracy: 0.9008\n",
      "870th iteration, loss: 0.4885, test accuracy: 0.8926\n",
      "880th iteration, loss: 0.3261, test accuracy: 0.8908\n",
      "890th iteration, loss: 0.2864, test accuracy: 0.8975\n",
      "900th iteration, loss: 0.5648, test accuracy: 0.8915\n",
      "910th iteration, loss: 0.4171, test accuracy: 0.9008\n",
      "920th iteration, loss: 0.3642, test accuracy: 0.8773\n",
      "930th iteration, loss: 0.2753, test accuracy: 0.8960\n",
      "940th iteration, loss: 0.2730, test accuracy: 0.9108\n",
      "950th iteration, loss: 0.2055, test accuracy: 0.9005\n",
      "960th iteration, loss: 0.2171, test accuracy: 0.9129\n",
      "970th iteration, loss: 0.2491, test accuracy: 0.9052\n",
      "980th iteration, loss: 0.4741, test accuracy: 0.9001\n",
      "990th iteration, loss: 0.3552, test accuracy: 0.8999\n",
      "1000th iteration, loss: 0.3392, test accuracy: 0.9035\n",
      "1010th iteration, loss: 0.4547, test accuracy: 0.8905\n",
      "1020th iteration, loss: 0.3500, test accuracy: 0.9085\n",
      "1030th iteration, loss: 0.3232, test accuracy: 0.8671\n",
      "1040th iteration, loss: 0.4160, test accuracy: 0.9014\n",
      "1050th iteration, loss: 0.3868, test accuracy: 0.8970\n",
      "1060th iteration, loss: 0.3726, test accuracy: 0.8964\n",
      "1070th iteration, loss: 0.2725, test accuracy: 0.8919\n",
      "1080th iteration, loss: 0.2452, test accuracy: 0.8992\n",
      "1090th iteration, loss: 0.3510, test accuracy: 0.8980\n",
      "1100th iteration, loss: 0.3160, test accuracy: 0.9022\n",
      "1110th iteration, loss: 0.2153, test accuracy: 0.9058\n",
      "1120th iteration, loss: 0.3016, test accuracy: 0.8922\n",
      "1130th iteration, loss: 0.2686, test accuracy: 0.8972\n",
      "1140th iteration, loss: 0.4218, test accuracy: 0.9068\n",
      "1150th iteration, loss: 0.2623, test accuracy: 0.8984\n",
      "1160th iteration, loss: 0.5013, test accuracy: 0.8902\n",
      "1170th iteration, loss: 0.3004, test accuracy: 0.8947\n",
      "1180th iteration, loss: 0.2907, test accuracy: 0.8971\n",
      "1190th iteration, loss: 0.3230, test accuracy: 0.9048\n",
      "1200th iteration, loss: 0.4123, test accuracy: 0.9024\n",
      "1210th iteration, loss: 0.2488, test accuracy: 0.9035\n",
      "1220th iteration, loss: 0.3172, test accuracy: 0.9081\n",
      "1230th iteration, loss: 0.2355, test accuracy: 0.9092\n",
      "1240th iteration, loss: 0.2796, test accuracy: 0.9047\n",
      "1250th iteration, loss: 0.2562, test accuracy: 0.8973\n",
      "1260th iteration, loss: 0.5858, test accuracy: 0.8971\n",
      "1270th iteration, loss: 0.4570, test accuracy: 0.9018\n",
      "1280th iteration, loss: 0.2881, test accuracy: 0.8990\n",
      "1290th iteration, loss: 0.3635, test accuracy: 0.9088\n",
      "1300th iteration, loss: 0.3126, test accuracy: 0.9007\n",
      "1310th iteration, loss: 0.4895, test accuracy: 0.9074\n",
      "1320th iteration, loss: 0.3487, test accuracy: 0.9065\n",
      "1330th iteration, loss: 0.4151, test accuracy: 0.9080\n",
      "1340th iteration, loss: 0.3462, test accuracy: 0.8856\n",
      "1350th iteration, loss: 0.3844, test accuracy: 0.8990\n",
      "1360th iteration, loss: 0.5720, test accuracy: 0.8907\n",
      "1370th iteration, loss: 0.1669, test accuracy: 0.8977\n",
      "1380th iteration, loss: 0.4022, test accuracy: 0.9032\n",
      "1390th iteration, loss: 0.3406, test accuracy: 0.8910\n",
      "1400th iteration, loss: 0.1511, test accuracy: 0.8989\n",
      "1410th iteration, loss: 0.2483, test accuracy: 0.9034\n",
      "1420th iteration, loss: 0.4825, test accuracy: 0.9003\n",
      "1430th iteration, loss: 0.4325, test accuracy: 0.9050\n",
      "1440th iteration, loss: 0.2153, test accuracy: 0.9070\n",
      "1450th iteration, loss: 0.1392, test accuracy: 0.9124\n",
      "1460th iteration, loss: 0.2529, test accuracy: 0.9097\n",
      "1470th iteration, loss: 0.3232, test accuracy: 0.8997\n",
      "1480th iteration, loss: 0.1059, test accuracy: 0.9079\n",
      "1490th iteration, loss: 0.0964, test accuracy: 0.9100\n",
      "1500th iteration, loss: 0.4869, test accuracy: 0.9115\n",
      "1510th iteration, loss: 0.3372, test accuracy: 0.9085\n",
      "1520th iteration, loss: 0.5235, test accuracy: 0.8996\n",
      "1530th iteration, loss: 0.3768, test accuracy: 0.9110\n",
      "1540th iteration, loss: 0.2293, test accuracy: 0.8984\n",
      "1550th iteration, loss: 0.2327, test accuracy: 0.9077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1560th iteration, loss: 0.3297, test accuracy: 0.9005\n",
      "1570th iteration, loss: 0.1930, test accuracy: 0.9045\n",
      "1580th iteration, loss: 0.2883, test accuracy: 0.8977\n",
      "1590th iteration, loss: 0.3810, test accuracy: 0.9018\n",
      "1600th iteration, loss: 0.2813, test accuracy: 0.8986\n",
      "1610th iteration, loss: 0.2412, test accuracy: 0.9094\n",
      "1620th iteration, loss: 0.3222, test accuracy: 0.9102\n",
      "1630th iteration, loss: 0.5376, test accuracy: 0.9105\n",
      "1640th iteration, loss: 0.4382, test accuracy: 0.8949\n",
      "1650th iteration, loss: 0.3904, test accuracy: 0.8954\n",
      "1660th iteration, loss: 0.2332, test accuracy: 0.9035\n",
      "1670th iteration, loss: 0.7232, test accuracy: 0.9067\n",
      "1680th iteration, loss: 0.4331, test accuracy: 0.8957\n",
      "1690th iteration, loss: 0.2846, test accuracy: 0.9071\n",
      "1700th iteration, loss: 0.2021, test accuracy: 0.8972\n",
      "1710th iteration, loss: 0.4566, test accuracy: 0.9021\n",
      "1720th iteration, loss: 0.2112, test accuracy: 0.9070\n",
      "1730th iteration, loss: 0.4279, test accuracy: 0.9019\n",
      "1740th iteration, loss: 0.3716, test accuracy: 0.9079\n",
      "1750th iteration, loss: 0.2850, test accuracy: 0.9098\n",
      "1760th iteration, loss: 0.2755, test accuracy: 0.9062\n",
      "1770th iteration, loss: 0.3445, test accuracy: 0.9122\n",
      "1780th iteration, loss: 0.3272, test accuracy: 0.9057\n",
      "1790th iteration, loss: 0.3727, test accuracy: 0.9022\n",
      "1800th iteration, loss: 0.2955, test accuracy: 0.9074\n",
      "1810th iteration, loss: 0.2721, test accuracy: 0.9092\n",
      "1820th iteration, loss: 0.2796, test accuracy: 0.9018\n",
      "1830th iteration, loss: 0.3478, test accuracy: 0.8950\n",
      "1840th iteration, loss: 0.1948, test accuracy: 0.9119\n",
      "1850th iteration, loss: 0.4397, test accuracy: 0.9063\n",
      "1860th iteration, loss: 0.3261, test accuracy: 0.9092\n",
      "1870th iteration, loss: 0.3440, test accuracy: 0.9075\n",
      "1880th iteration, loss: 0.2355, test accuracy: 0.9070\n",
      "1890th iteration, loss: 0.5195, test accuracy: 0.9051\n",
      "1900th iteration, loss: 0.3128, test accuracy: 0.9138\n",
      "1910th iteration, loss: 0.3735, test accuracy: 0.9083\n",
      "1920th iteration, loss: 0.3769, test accuracy: 0.9042\n",
      "1930th iteration, loss: 0.2643, test accuracy: 0.9070\n",
      "1940th iteration, loss: 0.5417, test accuracy: 0.9070\n",
      "1950th iteration, loss: 0.4138, test accuracy: 0.9024\n",
      "1960th iteration, loss: 0.5309, test accuracy: 0.9092\n",
      "1970th iteration, loss: 0.3257, test accuracy: 0.9075\n",
      "1980th iteration, loss: 0.3039, test accuracy: 0.9037\n",
      "1990th iteration, loss: 0.2929, test accuracy: 0.9131\n",
      "2000th iteration, loss: 0.1750, test accuracy: 0.9021\n",
      "2010th iteration, loss: 0.3140, test accuracy: 0.9065\n",
      "2020th iteration, loss: 0.5531, test accuracy: 0.9012\n",
      "2030th iteration, loss: 0.5741, test accuracy: 0.9140\n",
      "2040th iteration, loss: 0.3183, test accuracy: 0.8959\n",
      "2050th iteration, loss: 0.2724, test accuracy: 0.9086\n",
      "2060th iteration, loss: 0.1283, test accuracy: 0.9020\n",
      "2070th iteration, loss: 0.1767, test accuracy: 0.9030\n",
      "2080th iteration, loss: 0.4988, test accuracy: 0.9031\n",
      "2090th iteration, loss: 0.2585, test accuracy: 0.8952\n",
      "2100th iteration, loss: 0.6307, test accuracy: 0.9057\n",
      "2110th iteration, loss: 0.2505, test accuracy: 0.9099\n",
      "2120th iteration, loss: 0.5130, test accuracy: 0.9133\n",
      "2130th iteration, loss: 0.3540, test accuracy: 0.9075\n",
      "2140th iteration, loss: 0.4042, test accuracy: 0.9098\n",
      "2150th iteration, loss: 0.3277, test accuracy: 0.9047\n",
      "2160th iteration, loss: 0.3155, test accuracy: 0.9106\n",
      "2170th iteration, loss: 0.2579, test accuracy: 0.9079\n",
      "2180th iteration, loss: 0.4655, test accuracy: 0.9026\n",
      "2190th iteration, loss: 0.3821, test accuracy: 0.9065\n",
      "2200th iteration, loss: 0.1697, test accuracy: 0.9107\n",
      "2210th iteration, loss: 0.1741, test accuracy: 0.9090\n",
      "2220th iteration, loss: 0.2512, test accuracy: 0.8988\n",
      "2230th iteration, loss: 0.1822, test accuracy: 0.9099\n",
      "2240th iteration, loss: 0.6346, test accuracy: 0.9108\n",
      "2250th iteration, loss: 0.7574, test accuracy: 0.9092\n",
      "2260th iteration, loss: 0.2267, test accuracy: 0.9109\n",
      "2270th iteration, loss: 0.1545, test accuracy: 0.9079\n",
      "2280th iteration, loss: 0.1795, test accuracy: 0.9012\n",
      "2290th iteration, loss: 0.1786, test accuracy: 0.9028\n",
      "2300th iteration, loss: 0.1938, test accuracy: 0.9105\n",
      "2310th iteration, loss: 0.5600, test accuracy: 0.9095\n",
      "2320th iteration, loss: 0.3240, test accuracy: 0.8967\n",
      "2330th iteration, loss: 0.5555, test accuracy: 0.8922\n",
      "2340th iteration, loss: 0.5031, test accuracy: 0.9086\n",
      "2350th iteration, loss: 0.4085, test accuracy: 0.8957\n",
      "2360th iteration, loss: 0.4423, test accuracy: 0.9057\n",
      "2370th iteration, loss: 0.2126, test accuracy: 0.9034\n",
      "2380th iteration, loss: 0.4115, test accuracy: 0.9059\n",
      "2390th iteration, loss: 0.6127, test accuracy: 0.8959\n",
      "2400th iteration, loss: 0.4878, test accuracy: 0.8989\n",
      "2410th iteration, loss: 0.7480, test accuracy: 0.8880\n",
      "2420th iteration, loss: 0.3573, test accuracy: 0.9043\n",
      "2430th iteration, loss: 0.2833, test accuracy: 0.9042\n",
      "2440th iteration, loss: 0.3774, test accuracy: 0.9062\n",
      "2450th iteration, loss: 0.2526, test accuracy: 0.9133\n",
      "2460th iteration, loss: 0.2764, test accuracy: 0.8997\n",
      "2470th iteration, loss: 0.2784, test accuracy: 0.9011\n",
      "2480th iteration, loss: 0.2907, test accuracy: 0.9042\n",
      "2490th iteration, loss: 0.8300, test accuracy: 0.9096\n",
      "2500th iteration, loss: 0.2267, test accuracy: 0.9115\n",
      "2510th iteration, loss: 0.3509, test accuracy: 0.9062\n",
      "2520th iteration, loss: 0.3925, test accuracy: 0.9074\n",
      "2530th iteration, loss: 0.2558, test accuracy: 0.9112\n",
      "2540th iteration, loss: 0.2101, test accuracy: 0.9200\n",
      "2550th iteration, loss: 0.3808, test accuracy: 0.8957\n",
      "2560th iteration, loss: 0.2864, test accuracy: 0.9062\n",
      "2570th iteration, loss: 0.4690, test accuracy: 0.9030\n",
      "2580th iteration, loss: 0.3886, test accuracy: 0.9084\n",
      "2590th iteration, loss: 0.3396, test accuracy: 0.9092\n",
      "2600th iteration, loss: 0.3907, test accuracy: 0.9080\n",
      "2610th iteration, loss: 0.2051, test accuracy: 0.9134\n",
      "2620th iteration, loss: 0.1778, test accuracy: 0.9136\n",
      "2630th iteration, loss: 0.1744, test accuracy: 0.9114\n",
      "2640th iteration, loss: 0.4254, test accuracy: 0.9119\n",
      "2650th iteration, loss: 0.3455, test accuracy: 0.8983\n",
      "2660th iteration, loss: 0.2946, test accuracy: 0.8989\n",
      "2670th iteration, loss: 0.2548, test accuracy: 0.8990\n",
      "2680th iteration, loss: 0.1733, test accuracy: 0.9061\n",
      "2690th iteration, loss: 0.2939, test accuracy: 0.9006\n",
      "2700th iteration, loss: 0.2642, test accuracy: 0.9016\n",
      "2710th iteration, loss: 0.1557, test accuracy: 0.9010\n",
      "2720th iteration, loss: 0.2840, test accuracy: 0.9115\n",
      "2730th iteration, loss: 0.2186, test accuracy: 0.8987\n",
      "2740th iteration, loss: 0.2558, test accuracy: 0.9053\n",
      "2750th iteration, loss: 0.2978, test accuracy: 0.8964\n",
      "2760th iteration, loss: 0.2263, test accuracy: 0.9110\n",
      "2770th iteration, loss: 0.3789, test accuracy: 0.9125\n",
      "2780th iteration, loss: 0.2172, test accuracy: 0.9067\n",
      "2790th iteration, loss: 0.2956, test accuracy: 0.9148\n",
      "2800th iteration, loss: 0.2695, test accuracy: 0.9134\n",
      "2810th iteration, loss: 0.2071, test accuracy: 0.9040\n",
      "2820th iteration, loss: 0.1471, test accuracy: 0.9055\n",
      "2830th iteration, loss: 0.4600, test accuracy: 0.9097\n",
      "2840th iteration, loss: 0.1312, test accuracy: 0.9040\n",
      "2850th iteration, loss: 0.4292, test accuracy: 0.8959\n",
      "2860th iteration, loss: 0.3590, test accuracy: 0.9134\n",
      "2870th iteration, loss: 0.4783, test accuracy: 0.9015\n",
      "2880th iteration, loss: 0.2781, test accuracy: 0.9153\n",
      "2890th iteration, loss: 0.2747, test accuracy: 0.9154\n",
      "2900th iteration, loss: 0.3975, test accuracy: 0.9041\n",
      "2910th iteration, loss: 0.4143, test accuracy: 0.9129\n",
      "2920th iteration, loss: 0.1971, test accuracy: 0.9138\n",
      "2930th iteration, loss: 0.3076, test accuracy: 0.9133\n",
      "2940th iteration, loss: 0.1390, test accuracy: 0.9083\n",
      "2950th iteration, loss: 0.2411, test accuracy: 0.8919\n",
      "2960th iteration, loss: 0.1469, test accuracy: 0.9117\n",
      "2970th iteration, loss: 0.3500, test accuracy: 0.9029\n",
      "2980th iteration, loss: 0.2954, test accuracy: 0.9045\n",
      "2990th iteration, loss: 0.4351, test accuracy: 0.9104\n",
      "3000th iteration, loss: 0.4016, test accuracy: 0.8994\n",
      "3010th iteration, loss: 0.2119, test accuracy: 0.9125\n",
      "3020th iteration, loss: 0.1645, test accuracy: 0.9113\n",
      "3030th iteration, loss: 0.1292, test accuracy: 0.9000\n",
      "3040th iteration, loss: 0.4192, test accuracy: 0.8960\n",
      "3050th iteration, loss: 0.2970, test accuracy: 0.9080\n",
      "3060th iteration, loss: 0.6170, test accuracy: 0.9048\n",
      "3070th iteration, loss: 0.3737, test accuracy: 0.8992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3080th iteration, loss: 0.1771, test accuracy: 0.9117\n",
      "3090th iteration, loss: 0.2927, test accuracy: 0.8990\n",
      "3100th iteration, loss: 0.2298, test accuracy: 0.9106\n",
      "3110th iteration, loss: 0.1768, test accuracy: 0.8945\n",
      "3120th iteration, loss: 0.6634, test accuracy: 0.9072\n",
      "3130th iteration, loss: 0.4138, test accuracy: 0.9002\n",
      "3140th iteration, loss: 0.3034, test accuracy: 0.8973\n",
      "3150th iteration, loss: 0.3121, test accuracy: 0.8992\n",
      "3160th iteration, loss: 0.3090, test accuracy: 0.8968\n",
      "3170th iteration, loss: 0.2368, test accuracy: 0.8945\n",
      "3180th iteration, loss: 0.3661, test accuracy: 0.8958\n",
      "3190th iteration, loss: 0.3602, test accuracy: 0.9014\n",
      "3200th iteration, loss: 0.5323, test accuracy: 0.9023\n",
      "3210th iteration, loss: 0.2000, test accuracy: 0.8961\n",
      "3220th iteration, loss: 0.1973, test accuracy: 0.9047\n",
      "3230th iteration, loss: 0.2946, test accuracy: 0.9022\n",
      "3240th iteration, loss: 0.4499, test accuracy: 0.9007\n",
      "3250th iteration, loss: 0.3114, test accuracy: 0.9132\n",
      "3260th iteration, loss: 0.2763, test accuracy: 0.9031\n",
      "3270th iteration, loss: 0.2551, test accuracy: 0.9083\n",
      "3280th iteration, loss: 0.2272, test accuracy: 0.8984\n",
      "3290th iteration, loss: 0.4710, test accuracy: 0.9108\n",
      "3300th iteration, loss: 0.3142, test accuracy: 0.9095\n",
      "3310th iteration, loss: nan, test accuracy: 0.0980\n",
      "3320th iteration, loss: nan, test accuracy: 0.0980\n",
      "3330th iteration, loss: nan, test accuracy: 0.0980\n",
      "3340th iteration, loss: nan, test accuracy: 0.0980\n",
      "3350th iteration, loss: nan, test accuracy: 0.0980\n",
      "3360th iteration, loss: nan, test accuracy: 0.0980\n",
      "3370th iteration, loss: nan, test accuracy: 0.0980\n",
      "3380th iteration, loss: nan, test accuracy: 0.0980\n",
      "3390th iteration, loss: nan, test accuracy: 0.0980\n",
      "3400th iteration, loss: nan, test accuracy: 0.0980\n",
      "3410th iteration, loss: nan, test accuracy: 0.0980\n",
      "3420th iteration, loss: nan, test accuracy: 0.0980\n",
      "3430th iteration, loss: nan, test accuracy: 0.0980\n",
      "3440th iteration, loss: nan, test accuracy: 0.0980\n",
      "3450th iteration, loss: nan, test accuracy: 0.0980\n",
      "3460th iteration, loss: nan, test accuracy: 0.0980\n",
      "3470th iteration, loss: nan, test accuracy: 0.0980\n",
      "3480th iteration, loss: nan, test accuracy: 0.0980\n",
      "3490th iteration, loss: nan, test accuracy: 0.0980\n",
      "3500th iteration, loss: nan, test accuracy: 0.0980\n",
      "3510th iteration, loss: nan, test accuracy: 0.0980\n",
      "3520th iteration, loss: nan, test accuracy: 0.0980\n",
      "3530th iteration, loss: nan, test accuracy: 0.0980\n",
      "3540th iteration, loss: nan, test accuracy: 0.0980\n",
      "3550th iteration, loss: nan, test accuracy: 0.0980\n",
      "3560th iteration, loss: nan, test accuracy: 0.0980\n",
      "3570th iteration, loss: nan, test accuracy: 0.0980\n",
      "3580th iteration, loss: nan, test accuracy: 0.0980\n",
      "3590th iteration, loss: nan, test accuracy: 0.0980\n",
      "3600th iteration, loss: nan, test accuracy: 0.0980\n",
      "3610th iteration, loss: nan, test accuracy: 0.0980\n",
      "3620th iteration, loss: nan, test accuracy: 0.0980\n",
      "3630th iteration, loss: nan, test accuracy: 0.0980\n",
      "3640th iteration, loss: nan, test accuracy: 0.0980\n",
      "3650th iteration, loss: nan, test accuracy: 0.0980\n",
      "3660th iteration, loss: nan, test accuracy: 0.0980\n",
      "3670th iteration, loss: nan, test accuracy: 0.0980\n",
      "3680th iteration, loss: nan, test accuracy: 0.0980\n",
      "3690th iteration, loss: nan, test accuracy: 0.0980\n",
      "3700th iteration, loss: nan, test accuracy: 0.0980\n",
      "3710th iteration, loss: nan, test accuracy: 0.0980\n",
      "3720th iteration, loss: nan, test accuracy: 0.0980\n",
      "3730th iteration, loss: nan, test accuracy: 0.0980\n",
      "3740th iteration, loss: nan, test accuracy: 0.0980\n",
      "3750th iteration, loss: nan, test accuracy: 0.0980\n",
      "3760th iteration, loss: nan, test accuracy: 0.0980\n",
      "3770th iteration, loss: nan, test accuracy: 0.0980\n",
      "3780th iteration, loss: nan, test accuracy: 0.0980\n",
      "3790th iteration, loss: nan, test accuracy: 0.0980\n",
      "3800th iteration, loss: nan, test accuracy: 0.0980\n",
      "3810th iteration, loss: nan, test accuracy: 0.0980\n",
      "3820th iteration, loss: nan, test accuracy: 0.0980\n",
      "3830th iteration, loss: nan, test accuracy: 0.0980\n",
      "3840th iteration, loss: nan, test accuracy: 0.0980\n",
      "3850th iteration, loss: nan, test accuracy: 0.0980\n",
      "3860th iteration, loss: nan, test accuracy: 0.0980\n",
      "3870th iteration, loss: nan, test accuracy: 0.0980\n",
      "3880th iteration, loss: nan, test accuracy: 0.0980\n",
      "3890th iteration, loss: nan, test accuracy: 0.0980\n",
      "3900th iteration, loss: nan, test accuracy: 0.0980\n",
      "3910th iteration, loss: nan, test accuracy: 0.0980\n",
      "3920th iteration, loss: nan, test accuracy: 0.0980\n",
      "3930th iteration, loss: nan, test accuracy: 0.0980\n",
      "3940th iteration, loss: nan, test accuracy: 0.0980\n",
      "3950th iteration, loss: nan, test accuracy: 0.0980\n",
      "3960th iteration, loss: nan, test accuracy: 0.0980\n",
      "3970th iteration, loss: nan, test accuracy: 0.0980\n",
      "3980th iteration, loss: nan, test accuracy: 0.0980\n",
      "3990th iteration, loss: nan, test accuracy: 0.0980\n",
      "4000th iteration, loss: nan, test accuracy: 0.0980\n",
      "4010th iteration, loss: nan, test accuracy: 0.0980\n",
      "4020th iteration, loss: nan, test accuracy: 0.0980\n",
      "4030th iteration, loss: nan, test accuracy: 0.0980\n",
      "4040th iteration, loss: nan, test accuracy: 0.0980\n",
      "4050th iteration, loss: nan, test accuracy: 0.0980\n",
      "4060th iteration, loss: nan, test accuracy: 0.0980\n",
      "4070th iteration, loss: nan, test accuracy: 0.0980\n",
      "4080th iteration, loss: nan, test accuracy: 0.0980\n",
      "4090th iteration, loss: nan, test accuracy: 0.0980\n",
      "4100th iteration, loss: nan, test accuracy: 0.0980\n",
      "4110th iteration, loss: nan, test accuracy: 0.0980\n",
      "4120th iteration, loss: nan, test accuracy: 0.0980\n",
      "4130th iteration, loss: nan, test accuracy: 0.0980\n",
      "4140th iteration, loss: nan, test accuracy: 0.0980\n",
      "4150th iteration, loss: nan, test accuracy: 0.0980\n",
      "4160th iteration, loss: nan, test accuracy: 0.0980\n",
      "4170th iteration, loss: nan, test accuracy: 0.0980\n",
      "4180th iteration, loss: nan, test accuracy: 0.0980\n",
      "4190th iteration, loss: nan, test accuracy: 0.0980\n",
      "4200th iteration, loss: nan, test accuracy: 0.0980\n",
      "4210th iteration, loss: nan, test accuracy: 0.0980\n",
      "4220th iteration, loss: nan, test accuracy: 0.0980\n",
      "4230th iteration, loss: nan, test accuracy: 0.0980\n",
      "4240th iteration, loss: nan, test accuracy: 0.0980\n",
      "4250th iteration, loss: nan, test accuracy: 0.0980\n",
      "4260th iteration, loss: nan, test accuracy: 0.0980\n",
      "4270th iteration, loss: nan, test accuracy: 0.0980\n",
      "4280th iteration, loss: nan, test accuracy: 0.0980\n",
      "4290th iteration, loss: nan, test accuracy: 0.0980\n",
      "4300th iteration, loss: nan, test accuracy: 0.0980\n",
      "4310th iteration, loss: nan, test accuracy: 0.0980\n",
      "4320th iteration, loss: nan, test accuracy: 0.0980\n",
      "4330th iteration, loss: nan, test accuracy: 0.0980\n",
      "4340th iteration, loss: nan, test accuracy: 0.0980\n",
      "4350th iteration, loss: nan, test accuracy: 0.0980\n",
      "4360th iteration, loss: nan, test accuracy: 0.0980\n",
      "4370th iteration, loss: nan, test accuracy: 0.0980\n",
      "4380th iteration, loss: nan, test accuracy: 0.0980\n",
      "4390th iteration, loss: nan, test accuracy: 0.0980\n",
      "4400th iteration, loss: nan, test accuracy: 0.0980\n",
      "4410th iteration, loss: nan, test accuracy: 0.0980\n",
      "4420th iteration, loss: nan, test accuracy: 0.0980\n",
      "4430th iteration, loss: nan, test accuracy: 0.0980\n",
      "4440th iteration, loss: nan, test accuracy: 0.0980\n",
      "4450th iteration, loss: nan, test accuracy: 0.0980\n",
      "4460th iteration, loss: nan, test accuracy: 0.0980\n",
      "4470th iteration, loss: nan, test accuracy: 0.0980\n",
      "4480th iteration, loss: nan, test accuracy: 0.0980\n",
      "4490th iteration, loss: nan, test accuracy: 0.0980\n",
      "4500th iteration, loss: nan, test accuracy: 0.0980\n",
      "4510th iteration, loss: nan, test accuracy: 0.0980\n",
      "4520th iteration, loss: nan, test accuracy: 0.0980\n",
      "4530th iteration, loss: nan, test accuracy: 0.0980\n",
      "4540th iteration, loss: nan, test accuracy: 0.0980\n",
      "4550th iteration, loss: nan, test accuracy: 0.0980\n",
      "4560th iteration, loss: nan, test accuracy: 0.0980\n",
      "4570th iteration, loss: nan, test accuracy: 0.0980\n",
      "4580th iteration, loss: nan, test accuracy: 0.0980\n",
      "4590th iteration, loss: nan, test accuracy: 0.0980\n",
      "4600th iteration, loss: nan, test accuracy: 0.0980\n",
      "4610th iteration, loss: nan, test accuracy: 0.0980\n",
      "4620th iteration, loss: nan, test accuracy: 0.0980\n",
      "4630th iteration, loss: nan, test accuracy: 0.0980\n",
      "4640th iteration, loss: nan, test accuracy: 0.0980\n",
      "4650th iteration, loss: nan, test accuracy: 0.0980\n",
      "4660th iteration, loss: nan, test accuracy: 0.0980\n",
      "4670th iteration, loss: nan, test accuracy: 0.0980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4680th iteration, loss: nan, test accuracy: 0.0980\n",
      "4690th iteration, loss: nan, test accuracy: 0.0980\n",
      "4700th iteration, loss: nan, test accuracy: 0.0980\n",
      "4710th iteration, loss: nan, test accuracy: 0.0980\n",
      "4720th iteration, loss: nan, test accuracy: 0.0980\n",
      "4730th iteration, loss: nan, test accuracy: 0.0980\n",
      "4740th iteration, loss: nan, test accuracy: 0.0980\n",
      "4750th iteration, loss: nan, test accuracy: 0.0980\n",
      "4760th iteration, loss: nan, test accuracy: 0.0980\n",
      "4770th iteration, loss: nan, test accuracy: 0.0980\n",
      "4780th iteration, loss: nan, test accuracy: 0.0980\n",
      "4790th iteration, loss: nan, test accuracy: 0.0980\n",
      "4800th iteration, loss: nan, test accuracy: 0.0980\n",
      "4810th iteration, loss: nan, test accuracy: 0.0980\n",
      "4820th iteration, loss: nan, test accuracy: 0.0980\n",
      "4830th iteration, loss: nan, test accuracy: 0.0980\n",
      "4840th iteration, loss: nan, test accuracy: 0.0980\n",
      "4850th iteration, loss: nan, test accuracy: 0.0980\n",
      "4860th iteration, loss: nan, test accuracy: 0.0980\n",
      "4870th iteration, loss: nan, test accuracy: 0.0980\n",
      "4880th iteration, loss: nan, test accuracy: 0.0980\n",
      "4890th iteration, loss: nan, test accuracy: 0.0980\n",
      "4900th iteration, loss: nan, test accuracy: 0.0980\n",
      "4910th iteration, loss: nan, test accuracy: 0.0980\n",
      "4920th iteration, loss: nan, test accuracy: 0.0980\n",
      "4930th iteration, loss: nan, test accuracy: 0.0980\n",
      "4940th iteration, loss: nan, test accuracy: 0.0980\n",
      "4950th iteration, loss: nan, test accuracy: 0.0980\n",
      "4960th iteration, loss: nan, test accuracy: 0.0980\n",
      "4970th iteration, loss: nan, test accuracy: 0.0980\n",
      "4980th iteration, loss: nan, test accuracy: 0.0980\n",
      "4990th iteration, loss: nan, test accuracy: 0.0980\n",
      "5000th iteration, loss: nan, test accuracy: 0.0980\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Flatten data\n",
    "###################################################################\n",
    "#                    Implementation 3-1                           #\n",
    "###################################################################\n",
    "#train_data = None\n",
    "#test_data = None\n",
    "\n",
    "###################################################################\n",
    "#                    Implementation 3-2                           #\n",
    "###################################################################\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_true = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "###################################################################\n",
    "#                    Implementation 3-3                          #\n",
    "###################################################################\n",
    "h1 = fc('layer1', 512, x)\n",
    "y_logits = fc('layer2', 10, h1)\n",
    "y_hat = tf.nn.softmax(y_logits)\n",
    "\n",
    "###################################################################\n",
    "#                    Implementation 3-4                           #\n",
    "###################################################################\n",
    "cross_entropy = get_cross_entropy_loss(y_true, y_hat)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = get_accuracy(y_true, y_hat)\n",
    "# Make gradient descent op\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# Make op to initialize declared variable\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training\n",
    "    for step in range(max_iter):\n",
    "        # Get batch data and label\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # train the network and calculate cross entropy\n",
    "        _, loss = sess.run([train_step, cross_entropy], feed_dict={x: batch_x, y_true: batch_y})\n",
    "        # calcualte accuracy\n",
    "        acc = sess.run(accuracy, feed_dict={x: test_data, y_true: test_label})\n",
    "        # print loss (cross entropy) and accuracy at every 10th step\n",
    "        if (step + 1) % 10 == 0:\n",
    "            print(\"{}th iteration, loss: {:.4f}, test accuracy: {:.4f}\".format(step + 1, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
