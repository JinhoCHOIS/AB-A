{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-f2e855950638>, line 119)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-f2e855950638>\"\u001b[0;36m, line \u001b[0;32m119\u001b[0m\n\u001b[0;31m    emb_shape = # embedding shape (TODO)\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from builtins import ascii, bytes, chr, dict, filter, hex, input, int, map, next, oct, open, pow, range, round, str, \\\n",
    "  super, zip\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import timeit\n",
    "import collections\n",
    "import nltk\n",
    "\n",
    "TRAINING_SESSION = True\n",
    "\n",
    "# 변수들 설정\n",
    "rand = random.Random(0)\n",
    "embed_size = 256 # 워드 임베딩 차원 크기\n",
    "state_size = 256 # LSTM 히든 차원 크기\n",
    "max_epochs = 100 # 최대 학습 횟수\n",
    "minibatch_size = 20 # Minibatch 크기\n",
    "min_token_freq = 3 # 단어 최소 등장 빈도 최소 3번은 등장해야, 사전에 특정 단어 등록 \n",
    "\n",
    "run_start = timeit.default_timer() # 시간 측정\n",
    "\n",
    "print('Loading raw data...')\n",
    "nltk.download('brown') # nltk에서 brown 말뭉치 다운로드\n",
    "\n",
    "# all_seqs = [['and', 'how', 'right', 'she', 'was', '.'], ['the', 'operator', 'asked', 'pityingly', '.']]\n",
    "# brown 말뭉치 문장(길이가 x 이상 y이하) 추출 -> 추출된 문장의 단어들을 소문자화 후 list에 보관\n",
    "# all_seqs = [['i', 'am', 'a', 'boy],['you', 'are', 'a', 'girl']...]\n",
    "all_seqs = [[token.lower() for token in seq] for seq in nltk.corpus.brown.sents() if 5 <= len(seq) <= 30]\n",
    "rand.shuffle(all_seqs) # all_seqs 랜덤 섞기\n",
    "all_seqs = all_seqs[:50000] # all_seqs 중에 20000개만 사용\n",
    "\n",
    "\n",
    "trainingset_size = round(0.9 * len(all_seqs)) # 9할을 학습데이터로\n",
    "train_seqs = all_seqs[:trainingset_size] # 처음부터 trainingset_size까지를 train 데이터로\n",
    "validationset_size = len(all_seqs) - trainingset_size # 1할을 검증데이터로\n",
    "val_seqs = all_seqs[-validationset_size:] # 처음부터 validationset_size까지를 valid 데이터로\n",
    "\n",
    "print('Training set size:', trainingset_size)\n",
    "print('Validation set size:', validationset_size)\n",
    "\n",
    "all_tokens = (token for seq in train_seqs for token in seq) # all_tokens = ('and', 'how', 'right', ... )\n",
    "token_freqs = collections.Counter(all_tokens) # {'boy': freq, 'girl':freq, ....}\n",
    "vocab = sorted(token_freqs.keys(), key=lambda token: (-token_freqs[token], token)) # 단어 빈도수 정렬\n",
    "# 단어 최소 등장 빈도 이하인 단어들 모두 제거\n",
    "while token_freqs[vocab[-1]] < min_token_freq:\n",
    "    vocab.pop()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    vocab_size = len(vocab) + 2  # 문장 맨 앞, 맨 뒤 마커로 +1, 미등록어로 +1\n",
    "\n",
    "token_to_index = {token: i + 2 for (i, token) in enumerate(vocab)} # 단어에 고유 번호 부여\n",
    "index_to_token = {i + 2: token for (i, token) in enumerate(vocab)} # 고유 번호에 단어 부여\n",
    "edge_index = 0 # 문장 맨 앞, 맨 뒤 위치\n",
    "unknown_index = 1 # 미등록어 위치\n",
    "\n",
    "print('Vocabulary size:', vocab_size)\n",
    "\n",
    "# train, valid input을 생성하는 함수\n",
    "def parse(seqs):\n",
    "    indexes = list()\n",
    "    lens = list()\n",
    "    for seq in seqs:\n",
    "    # 문장 속 단어들을 모두 고유 번호로 변환\n",
    "        indexes_ = [token_to_index.get(token, unknown_index) for token in seq]\n",
    "        indexes.append(indexes_) # 고유 번호로 표현된 문장들 list에 추가\n",
    "        lens.append(len(indexes_) + 1)  # 모든 문장들의 길이 정보 저장 +1은 문장 맨 앞 혹은 맨 뒤 마커 추가 때문\n",
    "\n",
    "    maxlen = max(lens) # 가장 긴 문장의 길이 정보\n",
    "\n",
    "    in_mat = np.zeros((len(indexes), maxlen)) # LSTM 입력 문장 형태\n",
    "    out_mat = np.zeros((len(indexes), maxlen)) # LSTM 출력 문장 형태\n",
    "    for (row, indexes_) in enumerate(indexes):\n",
    "        in_mat[row, :len(indexes_) + 1] = [edge_index] + indexes_ # 문장의 시작에 edge 마커 추가\n",
    "        out_mat[row, :len(indexes_) + 1] = indexes_ + [edge_index] # 문장의 끝에 edge 마커 추가\n",
    "    return (in_mat, out_mat, np.array(lens))\n",
    "\n",
    "# train_seqs_in = [[0,423,23,8,656],[0,23,3,2,86]...]\n",
    "# train_seqs_out = [[423,23,8,656,0],[23,3,2,86,0]...]\n",
    "# train_seqs_len = [4,2,3,5, ...]\n",
    "(train_seqs_in, train_seqs_out, train_seqs_len) = parse(train_seqs)\n",
    "(val_seqs_in, val_seqs_out, val_seqs_len) = parse(val_seqs)\n",
    "\n",
    "print('Training set max length:', train_seqs_in.shape[1] - 1)\n",
    "print('Validation set max length:', val_seqs_in.shape[1] - 1)\n",
    "\n",
    "################################################################\n",
    "print()\n",
    "print('Training...')\n",
    "\n",
    "# Full correct sequence of token indexes with start token but without end token.\n",
    "# LSTM의 입력\n",
    "seq_in = tf.placeholder(tf.int32, shape=[None, None], name='seq_in')  # [seq, token]\n",
    "\n",
    "# Length of sequences in seq_in.\n",
    "seq_len = tf.placeholder(tf.int32, shape=[None], name='seq_len')  # [seq]\n",
    "tf.assert_equal(tf.shape(seq_in)[0], tf.shape(seq_len)[0])\n",
    "\n",
    "# Full correct sequence of token indexes without start token but with end token.\n",
    "# LSTM의 출력 즉 정답\n",
    "seq_target = tf.placeholder(tf.int32, shape=[None, None], name='seq_target')  # [seq, token]\n",
    "tf.assert_equal(tf.shape(seq_in), tf.shape(seq_target))\n",
    "\n",
    "batch_size = tf.shape(seq_in)[0]  # Number of sequences to process at once.\n",
    "num_steps = tf.shape(seq_in)[1]  # Number of tokens in generated sequence. # LSTM에서 순환의 횟수\n",
    "\n",
    "# Mask of which positions in the matrix of sequences are actual labels as opposed to padding.\n",
    "# 나중 학습 에러를 계산하기 위한 마스킹\n",
    "# tf.sequence_mask([1, 3, 2], 5)  # [[True, False, False, False, False],\n",
    "                                  #  [True, True, True, False, False],\n",
    "                                  #  [True, True, False, False, False]]\n",
    "token_mask = tf.cast(tf.sequence_mask(seq_len, num_steps), tf.float32)  # [seq, token]\n",
    "\n",
    "with tf.variable_scope('prefix_encoder'):\n",
    "  # Encode each sequence prefix into a vector.\n",
    "\n",
    "  # Embedding matrix for token vocabulary.\n",
    "  # 임베딩 변수 선언 초기화 값은 xavier initializer 사용\n",
    "    emb_shape = # embedding shape (TODO)\n",
    "    embeddings = tf.get_variable('embeddings', emb_shape, tf.float32,\n",
    "                               tf.contrib.layers.xavier_initializer())  # [vocabulary token, token feature]\n",
    "\n",
    "  # 3D tensor of tokens in sequences replaced with their corresponding embedding vector.\n",
    "  # seq_in [seq, token] -> [seq, token, emb_feature]로 lookup\n",
    "    embedded = # tf.nn.embedding_lookup()  (TODO)\n",
    "\n",
    "  # Use an LSTM to encode the generated prefix.\n",
    "  # LSTM의 초기 state 선언 LSTM initial state = (c_vec,h_vec) 튜플로 구성됨\n",
    "    init_state = # tf.contrib.rnn.LSTMStateTuple() (TODO)\n",
    "  \n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(state_size) # 순환 신경망의 Cell을 정의\n",
    "  # 실제 LSTM cell을 가지는 RNN 네트워크를 생성 return 값은 (output, state) 튜플\n",
    "    prefix_vectors = # tf.nn.dynamic_rnn()[0]  (TODO)\n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "  # Output a probability distribution over the token vocabulary (including the end token).\n",
    "  # 최종 생성 단어 확률 분포를 구하는 부분 (state, state_size)*(state_size, vocab_size) + b = (state, vocab_size)\n",
    "    W = tf.get_variable('W', [state_size, vocab_size], tf.float32, tf.contrib.layers.xavier_initializer())\n",
    "    b = tf.get_variable('b', [vocab_size], tf.float32, tf.zeros_initializer())\n",
    "    logits = tf.reshape(tf.matmul(tf.reshape(prefix_vectors, [-1, state_size]), W) + b,\n",
    "                      [batch_size, num_steps, vocab_size])\n",
    "    predictions = tf.nn.softmax(logits)  # [seq, prefix, token]\n",
    "\n",
    "# 내부적으로 softmax를 취한 뒤 loss를 계산 한다. 따라서 입력이 logits이다. * token_mask로 해당사항 없는 것들은 0으로 만듬\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=seq_target, logits=logits) * token_mask\n",
    "total_loss = tf.reduce_sum(losses) # 문장 내 모든 단어에서 발생하는 loss의 합\n",
    "train_step = tf.train.AdamOptimizer().minimize(total_loss)\n",
    "saver = tf.train.Saver() # 변수들을 저장하고 불러오기 위한 클래스 초기화\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "if TRAINING_SESSION:\n",
    "  # () 속의 텐서들을 계산하거나 또는 () 속의 오퍼레이션을 실행\n",
    "  # 따라서 그래프 내의 변수들을 초기화하는 오퍼레이션을 실행\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    print('epoch', 'val loss', 'duration', sep='\\t')\n",
    "\n",
    "    epoch_start = timeit.default_timer()\n",
    "\n",
    "    validation_loss = 0\n",
    "    for i in range(len(val_seqs) // minibatch_size):\n",
    "    # seq_in 0부터 minibatch_size 까지를 입력으로 i번째 minibatch를 수행\n",
    "    # feed_dict는 그래프의 입력값, total_loss는 최종 그래프에서 구하게 될 텐서\n",
    "        minibatch_validation_loss = sess.run(total_loss, feed_dict={\n",
    "        seq_in: val_seqs_in[i * minibatch_size:(i + 1) * minibatch_size],\n",
    "        seq_len: val_seqs_len[i * minibatch_size:(i + 1) * minibatch_size],\n",
    "        seq_target: val_seqs_out[i * minibatch_size:(i + 1) * minibatch_size],\n",
    "        })\n",
    "    # 모든 minibatch들의 loss를 더해주는 것\n",
    "    validation_loss += minibatch_validation_loss\n",
    "\n",
    "    print(0, round(validation_loss, 3), round(timeit.default_timer() - epoch_start), sep='\\t')\n",
    "          last_validation_loss = validation_loss\n",
    "  \n",
    "  # 최초로 초기화 된 변수들을 모델로 저장함\n",
    "    saver.save(sess, './model')\n",
    "\n",
    "  #trainingset_indexes = [0,1,2,3,4,...len(train_seqs)]\n",
    "    trainingset_indexes = list(range(len(train_seqs)))\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        epoch_start = timeit.default_timer()\n",
    "\n",
    "        rand.shuffle(trainingset_indexes)\n",
    "    for i in range(len(trainingset_indexes) // minibatch_size):\n",
    "      # validation하고 다르게 random suffle이 들어가서 아래 내용이 조금 달라진 것\n",
    "        minibatch_indexes = trainingset_indexes[i * minibatch_size:(i + 1) * minibatch_size]\n",
    "        a = sess.run([train_step, logits, seq_target], feed_dict={\n",
    "        seq_in: train_seqs_in[minibatch_indexes],\n",
    "        seq_len: train_seqs_len[minibatch_indexes],\n",
    "        seq_target: train_seqs_out[minibatch_indexes],\n",
    "        })\n",
    "\n",
    "    validation_loss = 0\n",
    "    for i in range(len(val_seqs) // minibatch_size):\n",
    "        minibatch_validation_loss = sess.run(total_loss, feed_dict={\n",
    "        seq_in: val_seqs_in[i * minibatch_size:(i + 1) * minibatch_size],\n",
    "        seq_len: val_seqs_len[i * minibatch_size:(i + 1) * minibatch_size],\n",
    "        seq_target: val_seqs_out[i * minibatch_size:(i + 1) * minibatch_size],\n",
    "        })\n",
    "        validation_loss += minibatch_validation_loss\n",
    "      \n",
    "    # early_stop 조건\n",
    "    if validation_loss > last_validation_loss:\n",
    "        break\n",
    "    last_validation_loss = validation_loss\n",
    "\n",
    "    saver.save(sess, './model')\n",
    "\n",
    "    print(epoch, round(validation_loss, 3), round(timeit.default_timer() - epoch_start), sep='\\t')\n",
    "\n",
    "    print(epoch, round(validation_loss, 3), round(timeit.default_timer() - epoch_start), sep='\\t')\n",
    "\n",
    "################################################################\n",
    "print()\n",
    "print('Evaluating...')\n",
    "\n",
    "# 저장된 모델 불러오는 것\n",
    "saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "### 실습 함수\n",
    "# def seq_prob(seq):\n",
    "#   return ?\n",
    "\n",
    "# print('P(the dog barked.) =', seq_prob(['the', 'dog', 'barked', '.']))\n",
    "# print('P(the cat barked.) =', seq_prob(['the', 'cat', 'barked', '.']))\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
